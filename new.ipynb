{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c7c9a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch \n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5650f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAILABLE_MODELS = {\n",
    "    \"google/flan-t5-base\": \"Flan-T5 Base (Recommended)\",\n",
    "    \"google/flan-t5-small\": \"Flan-T5 Small (Faster)\",\n",
    "    \n",
    "}\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "DB_FILE = 'faq_database.db'\n",
    "SIMILARITY_TH = 0.65\n",
    "DISCLAIMER = \"\\n\\n--- \\n *DISCLAIMER: This is for educational purposes only and not a substitute for professional medical advice.*\"\n",
    "\n",
    "PROMPT_TEMPLATE_DIRECT = \"\"\"\n",
    "Directly answer the user's question using the provided context.\n",
    "\n",
    "Context: {context}\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE_GOOD = \"\"\" \n",
    "You are a helpful and cautious Healthcare Information Assistant. Your task is to answer the user's question based *primarily* on the provided context.\n",
    "If the context contains the answer, use it to form a clear and concise response. If the context is not relevant, use your general knowledge to answer, but state that the information is general.\n",
    "Never provide a medical diagnosis or prescribe treatment.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "USER QUESTION:\n",
    "{query}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATES = {\n",
    "    \"Cautious Assistant\": PROMPT_TEMPLATE_GOOD,\n",
    "    \"Direct Assistant\": PROMPT_TEMPLATE_DIRECT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4fc5cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database \n",
    "\n",
    "def setup_database():\n",
    "    \"\"\"Makes a database if not present\"\"\"\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS faqs (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            question TEXT NOT NULL UNIQUE,\n",
    "            answer TEXT NOT NULL )\n",
    "                \"\"\")\n",
    "    cursor.execute(\"SELECT COUNT (*) FROM faqs\")\n",
    "\n",
    "    if cursor.fetchone()[0] == 0:\n",
    "        print(\"Database is empty. Population with initial data..\")\n",
    "        inital_data = [\n",
    "            (\"What are the common symptoms of the seasonal flu?\", \"Common flu symptoms include a high fever, body aches, headache, fatigue, a dry cough, and a sore throat. It's distinct from a cold as it comes on more suddenly and is more severe.\"),\n",
    "            (\"How can I differentiate between a cold and the flu?\", \"The flu is generally more severe than the common cold. Flu symptoms appear more suddenly and intensely, and are more likely to include fever and body aches, whereas a cold typically involves a runny or stuffy nose.\"),\n",
    "            (\"What is the proper first aid for a minor burn?\", \"For a minor burn, immediately cool the area by running it under cool (not cold) water for 10-20 minutes. Cover it with a sterile, non-adhesive bandage. Do not apply ice, butter, or oils.\"),\n",
    "            (\"How much water should an average adult drink per day?\", \"A general guideline is to drink about 8 glasses (around 2 liters or half a gallon) of water per day. However, needs can vary based on activity level, climate, and overall health.\"),\n",
    "            (\"What is the recommended amount of sleep for adults?\", \"Most adults require 7-9 hours of quality sleep per night for optimal physical health, cognitive function, and emotional well-being.\"),\n",
    "            (\"What are the hospital's standard operating hours?\", \"Our general clinic and outpatient services are open from 9 AM to 6 PM on weekdays. The emergency department remains open 24 hours a day, 7 days a week.\"),\n",
    "            (\"What is the contact number for medical emergencies?\", \"For any life-threatening medical emergency, please dial 911 immediately. For our hospital's dedicated emergency room, the direct line is (555) 765-4321.\"),\n",
    "        ]\n",
    "\n",
    "        cursor.executemany(\"INSERT INTO faqs (question, answer) VALUES (?, ?)\", inital_data)\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def add_faqs (question: str, answer: str) -> str:\n",
    "    \"\"\"Add new faq to database\"\"\"\n",
    "\n",
    "    if not question or not answer:\n",
    "        return \"Error : Question and Answer must not be empty\"\n",
    "    \n",
    "    try :\n",
    "        conn = sqlite3.connect(DB_FILE)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"INSERT INTO faqs (question, answer) VALUES (?, ?)\", (question, answer))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        return f\"Added : {question[:30]}... to database.\"\n",
    "    \n",
    "    except sqlite3.IntegrityError:\n",
    "        return \"Error : This quesiton already exists in the database.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05a044d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat-Bot pipleline \n",
    "\n",
    "class ChatBot :\n",
    "    def __init__(self, default_model=\"google/flan-t5-base\"):\n",
    "        print(\"Initializing Chatbot pipleline.... \")\n",
    "        self.device = \"mps\"\n",
    "        print(f\"Using device : {self.device}\")\n",
    "        self.embedding_model = SentenceTransformer(EMBEDDING_MODEL, device=self.device)\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.switch_model(default_model)\n",
    "\n",
    "        self.query_counts = {}\n",
    "        self.pending_for_review = {}\n",
    "        self.REVIEW_THRE = 3\n",
    "        self.refresh()\n",
    "\n",
    "    \n",
    "    def switch_model(self, model_name: str):\n",
    "        print(f\"Switching model to : {model_name}\")\n",
    "\n",
    "        try :\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "            print(\"Model swithched succesfully.\")\n",
    "            return f\"Switched to {model_name}\"\n",
    "        \n",
    "        except Exception as e :\n",
    "            return f\"Failed to switch model. Error : {e}\"\n",
    "\n",
    "    def refresh(self):\n",
    "        print('Refreshing.......')\n",
    "        conn = sqlite3.connect(DB_FILE)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('SELECT question, answer FROM faqs')\n",
    "        rows = cursor.fetchall()\n",
    "        conn.close()\n",
    "\n",
    "        if not rows:\n",
    "            print(\"Warning: Knowledge base is empty.\")\n",
    "            self.faqs_question = []\n",
    "            self.faqs_data_map = {}\n",
    "            self.faq_embeddings = torch.empty(0, device=self.device)\n",
    "            return \n",
    "        \n",
    "        self.faqs_question = [row[0] for row in rows]\n",
    "        self.faqs_data_map = {row[0]: row[1] for row in rows}\n",
    "\n",
    "        self.faq_embeddings = self.embedding_model.encode(\n",
    "            self.faqs_question,\n",
    "            convert_to_tensor = True,\n",
    "            show_progress_bar = False\n",
    "        ).to(self.device)\n",
    "        \n",
    "        print(f\"Database now contains : {len(self.faqs_question)} FAQs\")\n",
    "\n",
    "    def find_most_similar (self, user_query_embedding):\n",
    "        if len(user_query_embedding) == 0:\n",
    "            return None, 0.0\n",
    "        \n",
    "        similarities = util.pytorch_cos_sim(user_query_embedding, self.faq_embeddings)[0]\n",
    "        best_match_indx = torch.argmax(similarities).item()\n",
    "        best_match_score = similarities[best_match_indx].item()\n",
    "        return self.faqs_question[best_match_indx], best_match_score\n",
    "\n",
    "    def __call__(self, user_query: str, prompt_template_name: str) -> tuple[str, str, str, str] :\n",
    "\n",
    "        query_embedding = self.embedding_model.encode(user_query, convert_to_tensor=True).to(self.device)\n",
    "        matched_query, score = self.find_most_similar(query_embedding)\n",
    "        prompt_template = PROMPT_TEMPLATES.get(prompt_template_name, PROMPT_TEMPLATE_GOOD) # default templet is good\n",
    "\n",
    "\n",
    "        if score > SIMILARITY_TH :\n",
    "            source = \"FAQ Database\"\n",
    "            context = self.faqs_data_map[matched_query]\n",
    "            prompt = prompt_template.format(context = context, query = user_query)\n",
    "            matched_question = matched_query\n",
    "        \n",
    "        else :\n",
    "            source = \"Dr. Antik\"    # an alias for the the model , PS : he is my room-mate ;)\n",
    "            matched_question = \"N\\A\"\n",
    "            prompt = prompt_template.format(context = \"No specific context found.\", query = user_query)\n",
    "\n",
    "            normalized_query = user_query.lower().strip()\n",
    "            if normalized_query:\n",
    "                self.query_counts[normalized_query] = self.query_counts.get(normalized_query, 0) + 1\n",
    "\n",
    "                if self.query_counts[normalized_query] >= self.REVIEW_THRE :\n",
    "                    self.pending_for_review.append(normalized_query)\n",
    "                    print(f\"Question '{normalized_query}' has been flagged for review.\")\n",
    "\n",
    "                self.query_counts[normalized_query] = 0\n",
    "        \n",
    "        input = self.tokenizer(prompt, return_tensors = \"pt\").to(self.device)\n",
    "        output = self.model.generate(**input, max_length = 512)\n",
    "        response_text = self.tokenizer.decode(output[0], skip_special_tokens = True)\n",
    "\n",
    "        final_response = response_text + DISCLAIMER\n",
    "        confidence_str = f\"{score: .2f}\"\n",
    "\n",
    "        return final_response, source, matched_question, confidence_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a8560b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chatbot pipleline.... \n",
      "Using device : mps\n",
      "Switching model to : google/flan-t5-base\n",
      "Model swithched succesfully.\n",
      "Refreshing.......\n",
      "Database now contains : 7 FAQs\n"
     ]
    }
   ],
   "source": [
    "setup_database()\n",
    "pipeline = ChatBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI \n",
    "\n",
    "def handle_add_faqs (question, answer):\n",
    "    message = add_faqs(question, answer)\n",
    "\n",
    "    if \"Added\" in message :\n",
    "        pipeline.refresh()\n",
    "    \n",
    "    return message\n",
    "\n",
    "def get_pending_reviews():\n",
    "    if not pipeline.pending_for_review:\n",
    "        return \"No questions are currently pending review.\"\n",
    "    \n",
    "\n",
    "    review_list = \"### üìã Questions Flagged for Review:\\n\"\n",
    "    for i, question in enumerate(pipeline.pending_for_review, 1):\n",
    "        review_list += f\"{i}. `{question}`\\n\"\n",
    "    return review_list\n",
    "\n",
    "\n",
    "def handle_switch_model(model_name_from_ui):\n",
    "    model_path = [k for k, v in AVAILABLE_MODELS.items() if v == model_name_from_ui][0]\n",
    "    return pipeline.switch_model(model_path)\n",
    "\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as iface :\n",
    "    gr.Markdown(\"# Health Chat Bot\")\n",
    "\n",
    "    with gr.Tabs():\n",
    "        \n",
    "        with gr.TabItem(\"ChatBot\"):\n",
    "            gr.Markdown(\"This chatbot uses a **Retrieval-Augmented Generation (RAG)** pipeline to answer your questions.\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    question_box = gr.Textbox(lines = 4, placeholder=\"e.g., How do I treat a small burn?\", label=\"Ask a Health Question\")\n",
    "                    \n",
    "                    prompt_selector = gr.Radio(\n",
    "                        label=\"Select a Prompt Template\",\n",
    "                        choices=list(PROMPT_TEMPLATES.keys()),\n",
    "                        value=\"Cautious Assistant\"\n",
    "                    )\n",
    "                    \n",
    "                    submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "\n",
    "\n",
    "                with gr.Column(scale=1):\n",
    "                    examples = gr.Examples(examples=[\"How do I know if I have the flu?\", \"what's the number for emergencies?\", \"how much sleep should i get?\"], inputs=[question_box])\n",
    "\n",
    "                    gr.Markdown(\"### ‚öôÔ∏è Model Selection\")\n",
    "                    model_selector = gr.Dropdown(\n",
    "                        label=\"Switch AI Model\",\n",
    "                        choices=list(AVAILABLE_MODELS.values()),\n",
    "                        value=AVAILABLE_MODELS[pipeline.model.name_or_path] # Set default\n",
    "                    )\n",
    "\n",
    "                    switch_model_btn = gr.Button(\"Apply Model Switch\")\n",
    "                    model_status_box = gr.Textbox(label=\"Status\", interactive=False)\n",
    "\n",
    "            gr.Markdown(\"### Response Details\")\n",
    "            with gr.Row():\n",
    "                source_label = gr.Label(label=\"Response Source\")\n",
    "                matched_question_label = gr.Label(label=\"Matched FAQ Question\")\n",
    "                confidence_label = gr.Label(label=\"Confidence Score\")\n",
    "            answer_box = gr.Textbox(lines=6, label=\"Chatbot Answer\", interactive=False)\n",
    "            \n",
    "            submit_btn.click(\n",
    "                fn=pipeline, \n",
    "                inputs=[question_box, prompt_selector], \n",
    "                outputs=[answer_box, source_label, matched_question_label, confidence_label]\n",
    "            )\n",
    "\n",
    "            switch_model_btn.click(\n",
    "                fn=handle_switch_model,\n",
    "                inputs=[model_selector],\n",
    "                outputs=[model_status_box]\n",
    "            )\n",
    "\n",
    "        with gr.TabItem(\"Manage Knowledge Base\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"## Add a New FAQ\")\n",
    "                    new_question_box = gr.Textbox(lines=2, label=\"New Question\")\n",
    "                    new_answer_box = gr.Textbox(lines=5, label=\"New Answer\")\n",
    "                    add_faq_btn = gr.Button(\"Add New FAQ\", variant=\"primary\")\n",
    "                    add_status_box = gr.Textbox(label=\"Status\", interactive=False)\n",
    "\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"## ‚úçÔ∏è Pending Review\")\n",
    "                    pending_review_display = gr.Markdown(value=\"Click 'Refresh' to see pending questions.\")\n",
    "                    refresh_review_btn = gr.Button(\"Refresh Review List\")\n",
    "            \n",
    "            add_faq_btn.click(\n",
    "                fn=handle_add_faqs,\n",
    "                inputs=[new_question_box, new_answer_box],\n",
    "                outputs=[add_status_box]\n",
    "            )\n",
    "\n",
    "            refresh_review_btn.click(\n",
    "                fn=get_pending_reviews,\n",
    "                inputs=None,\n",
    "                outputs=[pending_review_display]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9f1e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switching model to : google/flan-t5-base\n",
      "Model swithched succesfully.\n"
     ]
    }
   ],
   "source": [
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
